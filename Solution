import numpy as np
import tensorflow as tf
from keras.datasets import mnist
import matplotlib.pyplot as plt

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize pixel values
X_train = X_train.reshape(-1, 784) / 255.0
X_test = X_test.reshape(-1, 784) / 255.0

# Convert labels to one-hot encoding
y_train_onehot = tf.one_hot(y_train, depth=10).numpy()

# Initialize weights using He Initialization
def initialize_weights(input_size, hidden_size, output_size):
    mean = 0
    sigma_sq = 2 / input_size
    W1 = np.random.normal(loc=mean, scale=np.sqrt(sigma_sq), size=(input_size, hidden_size))
    b1 = np.zeros((1, hidden_size))
    W2 = np.random.normal(loc=mean, scale=np.sqrt(sigma_sq), size=(hidden_size, output_size))
    b2 = np.zeros((1, output_size))
    return W1, b1, W2, b2

# Define activation functions
def relu(x):
    return np.maximum(0, x)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

# Forward propagation
def forward_propagation(X, W1, b1, W2, b2):
    Z1 = np.dot(X, W1) + b1
    A1 = relu(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

# Loss function
def compute_loss(Y, Y_pred):
    m = Y.shape[0]
    loss = -1 / m * np.sum(Y * np.log(Y_pred))
    return loss

# Backpropagation using the derived equations
def backward_propagation(X, Y, Z1, A1, Z2, A2, W1, W2, b1, b2):
    m = X.shape[0]
    dZ2 = A2 - Y
    dW2 = 1/m * np.dot(A1.T, dZ2)
    db2 = 1/m * np.sum(dZ2, axis=0, keepdims=True)
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * (Z1 > 0)
    dW1 = 1/m * np.dot(X.T, dZ1)
    db1 = 1/m * np.sum(dZ1, axis=0, keepdims=True)
    return dW1, db1, dW2, db2

# Update parameters using derived equations
def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    return W1, b1, W2, b2

# Define the learning rate scheduler
def cosine_annealing_learning_rate(learning_rate_initial, max_epochs, decay_factor, epoch):
    cosine_decay = 0.5 * (1 + np.cos(np.pi * epoch / max_epochs))
    return (1 - decay_factor) * cosine_decay + decay_factor * learning_rate_initial

# Training function with batch size
def train(X, Y, hidden_size, output_size, batch_size, max_epochs, learning_rate_initial, decay_factor):
    input_size = X.shape[1]
    W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)
    num_batches = X.shape[0] // batch_size
    losses = []
    accuracies = []
    
    for epoch in range(max_epochs):
        for batch in range(num_batches):
            start = batch * batch_size
            end = (batch + 1) * batch_size
            X_batch = X[start:end]
            Y_batch = Y[start:end]
            
            # Forward propagation
            Z1, A1, Z2, A2 = forward_propagation(X_batch, W1, b1, W2, b2)
            
            # Compute loss
            loss = compute_loss(Y_batch, A2)
            losses.append(loss)
            
            # Backpropagation
            dW1, db1, dW2, db2 = backward_propagation(X_batch, Y_batch, Z1, A1, Z2, A2, W1, W2, b1, b2)
            
            # Update parameters
            learning_rate = cosine_annealing_learning_rate(learning_rate_initial, max_epochs, decay_factor, epoch)
            W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)
        
        # Compute accuracy
        _, _, _, A_train = forward_propagation(X_train, W1, b1, W2, b2)
        accuracy = np.mean(np.argmax(A_train, axis=1) == y_train)
        accuracies.append(accuracy)
        
        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch + 1}/{max_epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}")
    
    return W1, b1, W2, b2, losses, accuracies

# Set hyperparameters (needs to be tuned for best accuracy)
hidden_size = 64
output_size = 10
batch_size = 64
max_epochs = 50
learning_rate_initial = 0.001
decay_factor = 0.1

# Train the model
trained_weights, losses, accuracies = train(X_train, y_train_onehot, hidden_size, output_size, batch_size, max_epochs, learning_rate_initial, decay_factor)

# Plot loss and accuracy (did not use in the end)
epochs = range(1, max_epochs + 1)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, losses, label='Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, accuracies, label='Accuracy', color='green')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy over Epochs')
plt.legend()

plt.show()
